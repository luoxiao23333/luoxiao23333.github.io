<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>DS-GA-1003 Lab2 Lasso and Coordinate Descent | Always on the way</title><meta name="author" content="Xiao L.,lxiao70@gatech.edu"><meta name="copyright" content="Xiao L."><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="This is my lab from DS-GA-1003 NYUI put all my code in my github Import and Construct Dataset123456# Importimport numpy as npfrom scipy.optimize import minimizeimport pandas as pdimport matplotlib.pyp">
<meta property="og:type" content="article">
<meta property="og:title" content="DS-GA-1003 Lab2 Lasso and Coordinate Descent">
<meta property="og:url" content="http://example.com/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab2%20Lasso%20and%20Coordinate%20Descent/index.html">
<meta property="og:site_name" content="Always on the way">
<meta property="og:description" content="This is my lab from DS-GA-1003 NYUI put all my code in my github Import and Construct Dataset123456# Importimport numpy as npfrom scipy.optimize import minimizeimport pandas as pdimport matplotlib.pyp">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/3.jpeg">
<meta property="article:published_time" content="2022-12-10T13:03:25.000Z">
<meta property="article:modified_time" content="2022-12-13T13:25:31.738Z">
<meta property="article:author" content="Xiao L.">
<meta property="article:tag" content="Note">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/3.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab2%20Lasso%20and%20Coordinate%20Descent/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'DS-GA-1003 Lab2 Lasso and Coordinate Descent',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-12-13 08:25:31'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">82</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-home"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/LogseqNote/index.html#/page/Papers"><i class="fa-fw fas fa-sticky-note"></i><span> Research Note</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> Courses</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/GraduateCourses/"><i class="fa-fw fas fa-book"></i><span> Graduate Courses</span></a></li><li><a class="site-page child" href="/OnlineCourses/"><i class="fa-fw fas fa-book"></i><span> Online Courses</span></a></li><li><a class="site-page child" href="/UndergraduateCourses/"><i class="fa-fw fas fa-book"></i><span> Undergraduate</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/Link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/3.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Always on the way</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-home"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/LogseqNote/index.html#/page/Papers"><i class="fa-fw fas fa-sticky-note"></i><span> Research Note</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> Courses</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/GraduateCourses/"><i class="fa-fw fas fa-book"></i><span> Graduate Courses</span></a></li><li><a class="site-page child" href="/OnlineCourses/"><i class="fa-fw fas fa-book"></i><span> Online Courses</span></a></li><li><a class="site-page child" href="/UndergraduateCourses/"><i class="fa-fw fas fa-book"></i><span> Undergraduate</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/Link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">DS-GA-1003 Lab2 Lasso and Coordinate Descent</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-12-10T13:03:25.000Z" title="Created 2022-12-10 08:03:25">2022-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-12-13T13:25:31.738Z" title="Updated 2022-12-13 08:25:31">2022-12-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Course-Note/">Course Note</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Course-Note/DS-DA-1003-Machine-Learning/">DS-DA-1003 Machine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="DS-GA-1003 Lab2 Lasso and Coordinate Descent"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>This is my lab from <a target="_blank" rel="noopener" href="https://davidrosenberg.github.io/ml2016/#lectures">DS-GA-1003 NYU</a><br>I put all my code in my <a target="_blank" rel="noopener" href="https://github.com/luoxiao23333/DS-GA-1003">github</a></p>
<h1 id="Import-and-Construct-Dataset"><a href="#Import-and-Construct-Dataset" class="headerlink" title="Import and Construct Dataset"></a>Import and Construct Dataset</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Construct Data</span></span><br><span class="line">root_dir: <span class="built_in">str</span> = <span class="string">&quot;data//&quot;</span></span><br><span class="line">file_ext: <span class="built_in">str</span> = <span class="string">&quot;.csv&quot;</span></span><br><span class="line">delimiter: <span class="built_in">str</span> = <span class="string">&quot;,&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_path</span>(<span class="params">file_name: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="keyword">return</span> root_dir + file_name + file_ext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_csv</span>(<span class="params">file_name, data: np.ndarray, delimiter</span>):</span><br><span class="line">    frame = pd.DataFrame(data)</span><br><span class="line">    frame.to_csv(get_path(file_name), index=<span class="literal">False</span>, sep=delimiter,</span><br><span class="line">                 float_format=<span class="string">&quot;%.17f&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_csv</span>(<span class="params">file_name, delimiter</span>) -&gt; np.ndarray:</span><br><span class="line">    <span class="keyword">return</span> np.genfromtxt(get_path(file_name), delimiter=delimiter)[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">construct_dataset</span>():</span><br><span class="line">    m = <span class="number">150</span>  <span class="comment"># data rows</span></span><br><span class="line">    d = <span class="number">75</span>  <span class="comment"># feature dimensions</span></span><br><span class="line">    X: np.ndarray = np.random.rand(m, d)</span><br><span class="line"></span><br><span class="line">    theta = np.zeros(shape=(d, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    theta[:<span class="number">10</span>] = np.array([<span class="number">10</span> <span class="keyword">if</span> np.random.randint(<span class="number">0</span>, <span class="number">2</span>) == <span class="number">0</span> <span class="keyword">else</span> -<span class="number">10</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]).reshape((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    epsilon = np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">0.1</span>, size=(m, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    y: np.ndarray = np.dot(X, theta) + epsilon</span><br><span class="line"></span><br><span class="line">    save_csv(<span class="string">&quot;X_train&quot;</span>, X[:<span class="number">80</span>], delimiter=delimiter)</span><br><span class="line">    save_csv(<span class="string">&quot;X_validation&quot;</span>, X[<span class="number">80</span>:<span class="number">100</span>], delimiter=delimiter)</span><br><span class="line">    save_csv(<span class="string">&quot;X_test&quot;</span>, X[<span class="number">100</span>:], delimiter=delimiter)</span><br><span class="line">    save_csv(<span class="string">&quot;Y_train&quot;</span>, y[:<span class="number">80</span>], delimiter=delimiter)</span><br><span class="line">    save_csv(<span class="string">&quot;Y_validation&quot;</span>, y[<span class="number">80</span>:<span class="number">100</span>], delimiter=delimiter)</span><br><span class="line">    save_csv(<span class="string">&quot;Y_test&quot;</span>, y[<span class="number">100</span>:], delimiter=delimiter)</span><br><span class="line">    save_csv(<span class="string">&quot;Origin_theta&quot;</span>, theta, delimiter=delimiter)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reload data</span></span><br><span class="line">construct_dataset()</span><br><span class="line">X_train: np.ndarray = read_csv(<span class="string">&quot;X_train&quot;</span>, delimiter=delimiter)</span><br><span class="line">X_valid: np.ndarray = read_csv(<span class="string">&quot;X_validation&quot;</span>, delimiter=delimiter)</span><br><span class="line">X_test: np.ndarray = read_csv(<span class="string">&quot;X_test&quot;</span>, delimiter=delimiter)</span><br><span class="line">y_train: np.ndarray = read_csv(<span class="string">&quot;Y_train&quot;</span>, delimiter=delimiter)</span><br><span class="line">y_valid: np.ndarray = read_csv(<span class="string">&quot;Y_validation&quot;</span>, delimiter=delimiter)</span><br><span class="line">y_test: np.ndarray = read_csv(<span class="string">&quot;Y_test&quot;</span>, delimiter=delimiter)</span><br><span class="line">origin_theta: np.ndarray = read_csv(<span class="string">&quot;Origin_theta&quot;</span>, delimiter=delimiter)</span><br></pre></td></tr></table></figure>
<h1 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ridge_regression_test</span>(<span class="params">show_info: <span class="built_in">bool</span> = <span class="literal">True</span></span>) -&gt; [<span class="built_in">float</span>, np.ndarray]:</span><br><span class="line">    num_train_record, num_feature = X_train.shape</span><br><span class="line">    num_test_record = X_test.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ridge</span>(<span class="params">Lambda</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">ridge_obj</span>(<span class="params">obj_theta</span>):</span><br><span class="line">            <span class="keyword">return</span> ((np.linalg.norm(np.dot(X_train, obj_theta) - y_train)) ** <span class="number">2</span>) \</span><br><span class="line">                   / (<span class="number">2</span> * num_train_record) + Lambda * (np.linalg.norm(obj_theta)) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ridge_obj</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_loss</span>(<span class="params">theta</span>):</span><br><span class="line">        <span class="keyword">return</span> ((np.linalg.norm(np.dot(X_test, theta) - y_test)) ** <span class="number">2</span>) / (<span class="number">2</span> * num_test_record)</span><br><span class="line"></span><br><span class="line">    w = np.random.rand(num_feature)</span><br><span class="line"></span><br><span class="line">    min_lambda = <span class="number">0</span></span><br><span class="line">    min_loss = <span class="number">1e100</span></span><br><span class="line">    optimized_theta: np.ndarray</span><br><span class="line">    lambda_list = []</span><br><span class="line">    loss_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(-<span class="number">8</span>, <span class="number">3</span>):</span><br><span class="line">        Lambda = <span class="number">10</span> ** i</span><br><span class="line">        w_opt = minimize(ridge(Lambda), w)</span><br><span class="line">        loss = compute_loss(w_opt.x)</span><br><span class="line">        <span class="keyword">if</span> loss &lt; min_loss:</span><br><span class="line">            min_loss = loss</span><br><span class="line">            optimized_theta = w_opt.x</span><br><span class="line">            min_lambda = Lambda</span><br><span class="line">        lambda_list.append(Lambda)</span><br><span class="line">        loss_list.append(loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> show_info:</span><br><span class="line">        <span class="keyword">return</span> min_loss, optimized_theta</span><br><span class="line"></span><br><span class="line">    plt.plot(lambda_list, loss_list)</span><br><span class="line">    plt.xscale(<span class="string">&quot;log&quot;</span>)</span><br><span class="line">    plt.yscale(<span class="string">&quot;log&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;regelurization lambda&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;loss&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;loss over lambda in ridge regression&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Choose lambda: &#123;0&#125; \t\t min loss: &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(min_lambda, min_loss))</span><br><span class="line"></span><br><span class="line">    true_zero_count = <span class="number">0</span></span><br><span class="line">    thresh_hold = <span class="number">1e-3</span></span><br><span class="line">    small_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> ele <span class="keyword">in</span> optimized_theta:</span><br><span class="line">        <span class="keyword">if</span> ele == <span class="number">0</span>:</span><br><span class="line">            true_zero_count = true_zero_count + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> ele &lt;= thresh_hold:</span><br><span class="line">            small_count = small_count + <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;True zero component number is &#123;0&#125;, &quot;</span></span><br><span class="line">          <span class="string">&quot;component smaller than &#123;1&#125; is &#123;2&#125;, &quot;</span></span><br><span class="line">          <span class="string">&quot;over &#123;3&#125; component&quot;</span></span><br><span class="line">          .<span class="built_in">format</span>(true_zero_count,</span><br><span class="line">                  thresh_hold,</span><br><span class="line">                  small_count,</span><br><span class="line">                  optimized_theta.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;final theta by ridge regression is: \n&quot;</span>, optimized_theta)</span><br><span class="line">    <span class="keyword">return</span> min_loss, optimized_theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ridge_regression_test()</span><br></pre></td></tr></table></figure>
<p><img src="output_5_0.png" alt="png"></p>
<pre><code>Choose lambda: 1e-05          min loss: 0.038756157295944296
True zero component number is 0, component smaller than 0.001 is 36, over 75 component
----------------
final theta by ridge regression is: 
 [-1.00669875e+01  9.99048451e+00  1.00581023e+01  9.96790978e+00
  9.98378104e+00  1.00903399e+01  9.97811413e+00 -9.82803134e+00
 -9.99286346e+00  1.01992064e+01  7.36955778e-03  1.20814149e-01
 -1.83929675e-02  6.90596761e-02 -1.73710915e-01  2.33854982e-03
 -6.50656547e-02  7.26566720e-02  2.24123548e-02 -9.15567746e-02
  1.04819340e-01 -1.55851304e-02 -5.61461703e-02 -9.67911697e-03
  1.06058312e-01  2.89555208e-02 -7.28535254e-02  2.59701249e-02
 -2.62084231e-02  1.43849955e-01 -3.53616749e-02 -1.75473461e-01
 -7.51922238e-02  1.35459169e-01 -8.91451617e-02  4.00378153e-03
  3.40823811e-01  5.60312488e-02  1.38556088e-02 -1.36157164e-01
  5.44188584e-02 -3.31987338e-02 -1.14361459e-01  4.05963467e-02
  8.55518917e-02 -1.39128187e-01 -9.61777060e-02 -1.63676623e-01
 -1.66148362e-01  8.64139923e-02 -1.91462867e-01 -8.36287339e-02
 -6.77456110e-03  1.43563543e-01 -9.06126775e-02  1.67814109e-02
 -2.90278395e-02  1.06787324e-02  2.18951255e-01 -1.94027965e-02
  5.41766322e-02 -1.83557737e-01 -2.48183456e-02  1.24028650e-01
  7.85072559e-02 -2.38627015e-02  3.39238635e-02  1.59234368e-01
 -5.37899238e-02  1.17024677e-02 -1.15936139e-01  3.07691626e-02
 -5.35674402e-02 -1.72990346e-01  6.73598936e-02]





(0.038756157295944296,
 array([-1.00669875e+01,  9.99048451e+00,  1.00581023e+01,  9.96790978e+00,
         9.98378104e+00,  1.00903399e+01,  9.97811413e+00, -9.82803134e+00,
        -9.99286346e+00,  1.01992064e+01,  7.36955778e-03,  1.20814149e-01,
        -1.83929675e-02,  6.90596761e-02, -1.73710915e-01,  2.33854982e-03,
        -6.50656547e-02,  7.26566720e-02,  2.24123548e-02, -9.15567746e-02,
         1.04819340e-01, -1.55851304e-02, -5.61461703e-02, -9.67911697e-03,
         1.06058312e-01,  2.89555208e-02, -7.28535254e-02,  2.59701249e-02,
        -2.62084231e-02,  1.43849955e-01, -3.53616749e-02, -1.75473461e-01,
        -7.51922238e-02,  1.35459169e-01, -8.91451617e-02,  4.00378153e-03,
         3.40823811e-01,  5.60312488e-02,  1.38556088e-02, -1.36157164e-01,
         5.44188584e-02, -3.31987338e-02, -1.14361459e-01,  4.05963467e-02,
         8.55518917e-02, -1.39128187e-01, -9.61777060e-02, -1.63676623e-01,
        -1.66148362e-01,  8.64139923e-02, -1.91462867e-01, -8.36287339e-02,
        -6.77456110e-03,  1.43563543e-01, -9.06126775e-02,  1.67814109e-02,
        -2.90278395e-02,  1.06787324e-02,  2.18951255e-01, -1.94027965e-02,
         5.41766322e-02, -1.83557737e-01, -2.48183456e-02,  1.24028650e-01,
         7.85072559e-02, -2.38627015e-02,  3.39238635e-02,  1.59234368e-01,
        -5.37899238e-02,  1.17024677e-02, -1.15936139e-01,  3.07691626e-02,
        -5.35674402e-02, -1.72990346e-01,  6.73598936e-02]))
</code></pre><h1 id="Coordinate-Descent-for-Lasso-Regression"><a href="#Coordinate-Descent-for-Lasso-Regression" class="headerlink" title="Coordinate Descent for Lasso Regression"></a>Coordinate Descent for Lasso Regression</h1><p>Optimization Goal: $\hat{\theta}=argmin_{\theta\in R^{d}}\sum_{i=1}^{m}(\theta^{T}x_i-y_i)+\lambda ||\theta||_1$<br><img src="attachment:4377a9ac-f928-479d-853c-f37cbc49833a.png" alt="image.png"><br>$w\ is\ \theta$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">coordinate_descent_lasso</span>(<span class="params">X: np.ndarray, y: np.ndarray,</span></span><br><span class="line"><span class="params">                             lam_reg: <span class="built_in">float</span>, tolerance: <span class="built_in">float</span> = <span class="number">1e-5</span>,</span></span><br><span class="line"><span class="params">                             max_step=<span class="number">1000</span></span>) -&gt; np.ndarray:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">soft</span>(<span class="params">a, delta</span>):</span><br><span class="line">        compo = <span class="built_in">abs</span>(a) - delta</span><br><span class="line">        <span class="keyword">if</span> compo &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> np.sign(a) * compo</span><br><span class="line"></span><br><span class="line">    converge: <span class="built_in">bool</span> = <span class="literal">False</span></span><br><span class="line">    current_step: <span class="built_in">int</span> = <span class="number">0</span></span><br><span class="line">    N, D = X.shape</span><br><span class="line">    theta = np.linalg.inv(np.dot(X.T, X) + (lam_reg * np.identity(D))) \</span><br><span class="line">        .dot(X.T).dot(y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">not</span> converge) <span class="keyword">and</span> (current_step &lt; max_step):</span><br><span class="line">        current_step = current_step + <span class="number">1</span></span><br><span class="line">        last_theta = np.copy(theta)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(D):</span><br><span class="line">            a_j = <span class="number">0</span></span><br><span class="line">            c_j = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">                x_ij = X[i, j]</span><br><span class="line">                a_j += x_ij * x_ij</span><br><span class="line">                c_j += X[i, j] * \</span><br><span class="line">                    (y[i] - np.dot(theta.T, X[i]) + np.dot(theta[j], x_ij))</span><br><span class="line"></span><br><span class="line">            c_j *= <span class="number">2</span></span><br><span class="line">            a_j *= <span class="number">2</span></span><br><span class="line">            theta[j] = soft(c_j / a_j, lam_reg / a_j)</span><br><span class="line"></span><br><span class="line">        diff = np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(theta - last_theta))</span><br><span class="line">        converge = diff &lt;= tolerance</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">not</span> converge) <span class="keyword">and</span> (current_step % <span class="number">100</span> != <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\rlambda is &#123;0:.1f&#125;\t\tdiff is &#123;1&#125;\tsteps:&#123;2&#125;/&#123;3&#125;&quot;</span></span><br><span class="line">              .<span class="built_in">format</span>(lam_reg, diff, current_step, max_step), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\r&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>
<h2 id="Compute-square-loss"><a href="#Compute-square-loss" class="headerlink" title="Compute square loss"></a>Compute square loss</h2><p>$\mathcal{L}(X, y)=\frac{1}{N}\sum_{i=1}^{N}(X_i*\theta - y_i)^{2}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_square_loss</span>(<span class="params">X: np.ndarray, y: np.ndarray, theta: np.ndarray</span>) \</span><br><span class="line">        -&gt; <span class="built_in">float</span>:</span><br><span class="line"></span><br><span class="line">    N = X.shape[<span class="number">0</span>]</span><br><span class="line">    diff = X.dot(theta) - y</span><br><span class="line">    loss = <span class="number">1.0</span> / N * np.dot(diff.T, diff)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="Find-min-loss"><a href="#Find-min-loss" class="headerlink" title="Find min loss"></a>Find min loss</h2><p>Find the lambda and theta of min square loss<br>Can find that lasso’s loss is less than ridge with the sparse input</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># return lambda and theta of minimized square loss error</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_min_loss</span>(<span class="params">X_train, y_train, X_valid, y_valid</span>) -&gt; [<span class="built_in">float</span>, np.ndarray]:</span><br><span class="line">    sqr_loss = []</span><br><span class="line">    lambda_list = np.arange(<span class="number">0.1</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</span><br><span class="line">    min_loss = <span class="number">1e100</span></span><br><span class="line">    min_theta: np.ndarray = &#123;&#125;</span><br><span class="line">    min_lambda: <span class="built_in">float</span> = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> lambda_reg <span class="keyword">in</span> lambda_list:</span><br><span class="line">        theta = coordinate_descent_lasso(X_train, y_train, lambda_reg)</span><br><span class="line">        loss = compute_square_loss(X_valid, y_valid, theta)</span><br><span class="line">        sqr_loss.append(loss)</span><br><span class="line">        <span class="keyword">if</span> min_loss &gt; loss:</span><br><span class="line">            min_loss = loss</span><br><span class="line">            min_theta = np.copy(theta)</span><br><span class="line">            min_lambda = lambda_reg</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;min loss: &#123;0&#125;, lambda:&#123;1:.1f&#125;&quot;</span>.<span class="built_in">format</span>(min_loss, min_lambda))</span><br><span class="line">    plt.plot(lambda_list, sqr_loss)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;lambda&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;square error&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;square error over lambda&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> min_lambda, min_theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">min_lambda, min_theta = find_min_loss(X_train, y_train, X_valid, y_valid)</span><br></pre></td></tr></table></figure>
<pre><code>lambda is 0.1        diff is 0.06008851755771311    steps:1000/1000
lambda is 0.2        diff is 0.06899627007686944    steps:1000/1000
lambda is 0.3        diff is 0.08325210089294723    steps:1000/1000
lambda is 0.4        diff is 0.09547926436898656    steps:1000/1000
lambda is 0.5        diff is 0.0648970367967063    steps:1000/1000
lambda is 0.6        diff is 0.04435037153736939    steps:1000/1000
lambda is 0.7        diff is 0.013912975533066193    steps:1000/1000
lambda is 0.8        diff is 0.0026355262109854036    steps:1000/1000
lambda is 0.9        diff is 0.0003978676094541049    steps:1000/1000
lambda is 1.0        diff is 5.149093572631713e-05    steps:1000/1000
min loss: 0.015307889774195045, lambda:0.7
</code></pre><p><img src="output_11_1.png" alt="png"></p>
<h3 id="Find-how-many-true-zero"><a href="#Find-how-many-true-zero" class="headerlink" title="Find how many true zero"></a>Find how many true zero</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">find_true_zero_count</span>(<span class="params">origin_theta: np.ndarray,</span></span><br><span class="line"><span class="params">                         min_theta: np.ndarray</span>) -&gt; [<span class="built_in">int</span>, <span class="built_in">int</span>]:</span><br><span class="line">    origin_count = <span class="number">0</span></span><br><span class="line">    min_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> ele <span class="keyword">in</span> origin_theta:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(ele) &lt;= <span class="number">1e-5</span>:</span><br><span class="line">            origin_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> ele <span class="keyword">in</span> min_theta:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(ele) &lt;= <span class="number">1e-5</span>:</span><br><span class="line">            min_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> origin_count, min_count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">origin_count, min_count = find_true_zero_count(origin_theta, min_theta)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;origin theta has &#123;0&#125; true zero, theta of min loss&#x27;s lambda has &quot;</span></span><br><span class="line">      <span class="string">&quot;&#123;1&#125; true zero with tolerance: &#123;2&#125;\n&quot;</span></span><br><span class="line">      <span class="string">&quot;&#123;3&#125; zero component are estimated as non-zero&quot;</span></span><br><span class="line">      .<span class="built_in">format</span>(origin_count, min_count, <span class="number">1e-5</span>, origin_count - min_count))</span><br></pre></td></tr></table></figure>
<pre><code>origin theta has 65 true zero, theta of min loss&#39;s lambda has 53 true zero with tolerance: 1e-05
12 zero component are estimated as non-zero
</code></pre><h2 id="Homotopy-Method"><a href="#Homotopy-Method" class="headerlink" title="Homotopy Method"></a>Homotopy Method</h2><p>Warm start with ridge regression should let lasso converge more quickly</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">warm_start_coordinate_descent</span>(<span class="params">X: np.ndarray, y: np.ndarray,</span></span><br><span class="line"><span class="params">                                  lam_reg: <span class="built_in">float</span>, tolerance: <span class="built_in">float</span> = <span class="number">1e-3</span>,</span></span><br><span class="line"><span class="params">                                  max_step=<span class="number">1000</span></span>) -&gt; np.ndarray:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">soft</span>(<span class="params">a, delta</span>):</span><br><span class="line">        compo = <span class="built_in">abs</span>(a) - delta</span><br><span class="line">        <span class="keyword">if</span> compo &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> np.sign(a) * compo</span><br><span class="line"></span><br><span class="line">    converge: <span class="built_in">bool</span> = <span class="literal">False</span></span><br><span class="line">    current_step: <span class="built_in">int</span> = <span class="number">0</span></span><br><span class="line">    N, D = X.shape</span><br><span class="line">    _, theta = ridge_regression_test(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">not</span> converge) <span class="keyword">and</span> (current_step &lt; max_step):</span><br><span class="line">        current_step = current_step + <span class="number">1</span></span><br><span class="line">        last_theta = np.copy(theta)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(D):</span><br><span class="line">            a_j = <span class="number">0</span></span><br><span class="line">            c_j = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">                x_ij = X[i, j]</span><br><span class="line">                a_j += x_ij * x_ij</span><br><span class="line">                c_j += X[i, j] * \</span><br><span class="line">                    (y[i] - np.dot(theta.T, X[i]) + np.dot(theta[j], x_ij))</span><br><span class="line"></span><br><span class="line">            c_j *= <span class="number">2</span></span><br><span class="line">            a_j *= <span class="number">2</span></span><br><span class="line">            theta[j] = soft(c_j / a_j, lam_reg / a_j)</span><br><span class="line"></span><br><span class="line">        diff = np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(theta - last_theta))</span><br><span class="line">        converge = diff &lt;= tolerance</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">not</span> converge) <span class="keyword">and</span> (current_step % <span class="number">100</span> != <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\rlambda is &#123;0:.1f&#125;\t\tdiff is &#123;1&#125;\tsteps:&#123;2&#125;/&#123;3&#125;&quot;</span></span><br><span class="line">              .<span class="built_in">format</span>(lam_reg, diff, current_step, max_step), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\r&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_min_loss_warm_start</span>(<span class="params">X_train, y_train, X_valid, y_valid</span>) \</span><br><span class="line">                        -&gt; [<span class="built_in">float</span>, np.ndarray]:</span><br><span class="line">    sqr_loss = []</span><br><span class="line">    lambda_list = np.arange(<span class="number">0.1</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</span><br><span class="line">    min_loss = <span class="number">1e100</span></span><br><span class="line">    min_theta: np.ndarray = &#123;&#125;</span><br><span class="line">    min_lambda: <span class="built_in">float</span> = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> lambda_reg <span class="keyword">in</span> lambda_list:</span><br><span class="line">        theta = warm_start_coordinate_descent(X_train, y_train, lambda_reg)</span><br><span class="line">        loss = compute_square_loss(X_valid, y_valid, theta)</span><br><span class="line">        sqr_loss.append(loss)</span><br><span class="line">        <span class="keyword">if</span> min_loss &gt; loss:</span><br><span class="line">            min_loss = loss</span><br><span class="line">            min_theta = np.copy(theta)</span><br><span class="line">            min_lambda = lambda_reg</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;min loss: &#123;0&#125;, lambda:&#123;1:.1f&#125;&quot;</span>.<span class="built_in">format</span>(min_loss, min_lambda))</span><br><span class="line">    <span class="keyword">return</span> min_lambda, min_theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_, _ = find_min_loss_warm_start(X_train, y_train, X_valid, y_valid)</span><br></pre></td></tr></table></figure>
<pre><code>lambda is 0.1        diff is 0.0009953931916886843    steps:835/1000
lambda is 0.2        diff is 0.000988153783945539    steps:700/10000
lambda is 0.3        diff is 0.000998791003312332    steps:576/10000
lambda is 0.4        diff is 0.0009936626477643623    steps:460/1000
lambda is 0.5        diff is 0.000993502247473554    steps:376/1000
lambda is 0.6        diff is 0.000989000375419477    steps:326/10000
lambda is 0.7        diff is 0.000989182650121108    steps:284/10000
lambda is 0.8        diff is 0.000982675865428887    steps:260/1000
lambda is 0.9        diff is 0.0009678978659599484    steps:222/1000
lambda is 1.0        diff is 0.0009944533872836038    steps:188/1000
min loss: 0.016342880999627007, lambda:0.9
</code></pre><h2 id="compare-time-between-warm-start-and-non-warm-start"><a href="#compare-time-between-warm-start-and-non-warm-start" class="headerlink" title="compare time between warm start and non warm start"></a>compare time between warm start and non warm start</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compare</span>():</span><br><span class="line">    origin_time = []</span><br><span class="line">    warm_start_time = []</span><br><span class="line">    lambda_list = [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>]</span><br><span class="line">    <span class="keyword">for</span> lambda_reg <span class="keyword">in</span> lambda_list:</span><br><span class="line"></span><br><span class="line">        time_start = time.time()</span><br><span class="line">        _ = warm_start_coordinate_descent(X_train, y_train, lambda_reg, tolerance=<span class="number">0.01</span>)</span><br><span class="line">        warm_start_time.append(time.time() - time_start)</span><br><span class="line"></span><br><span class="line">        time_start = time.time()</span><br><span class="line">        _ = coordinate_descent_lasso(X_train, y_train, lambda_reg, tolerance=<span class="number">0.01</span>)</span><br><span class="line">        origin_time.append(time.time() - time_start)</span><br><span class="line"></span><br><span class="line">    plt.plot(lambda_list, origin_time, label=<span class="string">&quot;without warm start&quot;</span>)</span><br><span class="line">    plt.plot(lambda_list, warm_start_time, label=<span class="string">&quot;warm start with ridge regression&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.xlabel(<span class="string">&quot;lambda&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;time cost(seconds)&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Performance analyse of warm start&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">compare()</span><br></pre></td></tr></table></figure>
<pre><code>lambda is 0.6        diff is 0.009898263871577587    steps:167/1000
lambda is 0.6        diff is 0.04435037153736939    steps:1000/1000
lambda is 0.7        diff is 0.009957528853492322    steps:148/1000
lambda is 0.7        diff is 0.013912975533067251    steps:1000/1000
lambda is 0.8        diff is 0.009924335276691317    steps:133/1000
lambda is 0.8        diff is 0.00980506755009518    steps:941/1000
lambda is 0.9        diff is 0.00911076495458803    steps:121/1000
lambda is 0.9        diff is 0.00997934261343577    steps:872/1000
</code></pre><p><img src="output_17_1.png" alt="png"></p>
<h2 id="vectorize-coordinate-descent"><a href="#vectorize-coordinate-descent" class="headerlink" title="vectorize coordinate descent"></a>vectorize coordinate descent</h2><p>Vectorie computation procedure by transfering it to matrix computation will improve performance significantly<br>Improve around 18x ~ 36x compared with above  </p>
<script type="math/tex; mode=display">c_j=2\sum_{i=1}^{n}x_{ij}(y_i-\theta^{T}x_i+\theta_jx_{ij})=2[x_j\cdot y - (x\theta)\cdot x_j+\theta_j||x_j||_2^2]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">coordinate_descent_lasso_vectorize</span>(<span class="params">X: np.ndarray, y: np.ndarray,</span></span><br><span class="line"><span class="params">                                       theta: np.ndarray = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                                       lam_reg: <span class="built_in">float</span> = <span class="number">0.7</span>,</span></span><br><span class="line"><span class="params">                                       tolerance: <span class="built_in">float</span> = <span class="number">1e-5</span>,</span></span><br><span class="line"><span class="params">                                       max_step=<span class="number">10000</span></span>) -&gt; np.ndarray:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">soft</span>(<span class="params">a, delta</span>):</span><br><span class="line">        <span class="keyword">return</span> np.sign(a)*<span class="built_in">max</span>(<span class="built_in">abs</span>(a) - delta, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    time_start = time.time()</span><br><span class="line"></span><br><span class="line">    converge: <span class="built_in">bool</span> = <span class="literal">False</span></span><br><span class="line">    current_step: <span class="built_in">int</span> = <span class="number">0</span></span><br><span class="line">    N, D = X.shape</span><br><span class="line">    <span class="keyword">if</span> theta <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        theta = np.linalg.inv(np.dot(X.T, X) + (lam_reg * np.identity(D))) \</span><br><span class="line">            .dot(X.T).dot(y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">not</span> converge) <span class="keyword">and</span> (current_step &lt; max_step):</span><br><span class="line">        current_step = current_step + <span class="number">1</span></span><br><span class="line">        last_theta = np.copy(theta)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(D):</span><br><span class="line">            aj = <span class="number">2</span>*np.dot(X[:, j].T, X[:, j])</span><br><span class="line">            cj = <span class="number">2</span>*(X[:, j].dot(y) - (np.dot(X, theta).dot(X[:, j]))</span><br><span class="line">                    + theta[j]*(X[:, j].T.dot(X[:, j])))</span><br><span class="line">            theta[j] = soft(cj/aj, lam_reg/aj)</span><br><span class="line"></span><br><span class="line">        diff = np.<span class="built_in">sum</span>(np. <span class="built_in">abs</span>(theta - last_theta))</span><br><span class="line">        converge = diff &lt;= tolerance</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">not</span> converge) <span class="keyword">and</span> (current_step % <span class="number">100</span> != <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\rlambda is &#123;0:.1f&#125;\t&quot;</span></span><br><span class="line">              <span class="string">&quot;steps:&#123;1&#125;/&#123;2&#125;\t\trun time:&#123;3:.4f&#125; seconds loss:&#123;4:.4f&#125;&quot;</span></span><br><span class="line">              .<span class="built_in">format</span>(lam_reg, current_step,</span><br><span class="line">                      max_step, time.time()-time_start,</span><br><span class="line">                      compute_square_loss(X_valid, y_valid, theta)), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\r&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compare_with_vectorize</span>():</span><br><span class="line">    ridge_loss, ridge_theta = ridge_regression_test(show_info=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;loss of ridge regression with optimized lambda is &#123;0:.4f&#125;&quot;</span></span><br><span class="line">          .<span class="built_in">format</span>(ridge_loss))</span><br><span class="line">    lam_list = [<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">1.0</span>]</span><br><span class="line">    warm_start_cost = []</span><br><span class="line">    non_warm_start_cost = []</span><br><span class="line">    <span class="keyword">for</span> lam_reg <span class="keyword">in</span> lam_list:</span><br><span class="line">        <span class="comment"># without warm start</span></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        coordinate_descent_lasso_vectorize(X_train, y_train, <span class="literal">None</span>, lam_reg)</span><br><span class="line">        non_warm_start_cost.append(time.time() - start_time)</span><br><span class="line">        <span class="comment"># with warm start</span></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        coordinate_descent_lasso_vectorize(X_train, y_train,</span><br><span class="line">                                           ridge_theta, lam_reg)</span><br><span class="line">        warm_start_cost.append(time.time() - start_time)</span><br><span class="line"></span><br><span class="line">    plt.plot(lam_list, non_warm_start_cost, label=<span class="string">&quot;without warm start&quot;</span>)</span><br><span class="line">    plt.plot(lam_list, warm_start_cost, label=<span class="string">&quot;warm start with ridge regression&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.xlabel(<span class="string">&quot;lambda&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;time cost(seconds)&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Performance analyse of vectorized Coordinate Descent &quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">compare_with_vectorize()</span><br></pre></td></tr></table></figure>
<pre><code>loss of ridge regression with optimized lambda is 0.0387
lambda is 0.5    steps:1845/10000        run time:1.0552 seconds loss:0.0177
lambda is 0.5    steps:793/10000        run time:0.4571 seconds loss:0.0177
lambda is 0.6    steps:1608/10000        run time:0.9302 seconds loss:0.0170
lambda is 0.6    steps:473/10000        run time:0.2681 seconds loss:0.0170
lambda is 0.7    steps:1451/10000        run time:0.8382 seconds loss:0.0165
lambda is 0.7    steps:393/10000        run time:0.2231 seconds loss:0.0165
lambda is 0.8    steps:1327/10000        run time:0.7582 seconds loss:0.0164
lambda is 0.8    steps:370/10000        run time:0.2100 seconds loss:0.0164
lambda is 0.9    steps:1157/10000        run time:0.6571 seconds loss:0.0164
lambda is 0.9    steps:263/10000        run time:0.1490 seconds loss:0.0164
lambda is 1.0    steps:1061/10000        run time:0.6041 seconds loss:0.0172
lambda is 1.0    steps:203/10000        run time:0.1160 seconds loss:0.0172
</code></pre><p><img src="output_19_1.png" alt="png"></p>
<h1 id="Derive-Coordinate-Descent"><a href="#Derive-Coordinate-Descent" class="headerlink" title="Derive Coordinate Descent"></a>Derive Coordinate Descent</h1><p><img src="attachment:889d92ec-23b4-411f-8af3-b793a9c9cb1f.png" alt="image.png"></p>
<h2 id="Q1"><a href="#Q1" class="headerlink" title="Q1"></a>Q1</h2><p>if $x=\vec{0},w=\vec{0}$</p>
<h2 id="Q2"><a href="#Q2" class="headerlink" title="Q2"></a>Q2</h2><p>define:<br><img src="attachment:8fbe68e4-c429-4deb-8920-0de5f987cb12.png" alt="image.png"></p>
<script type="math/tex; mode=display">let\ d(\theta_j)=\frac{\partial f(\theta_j)}{\partial \theta_j}=(a_j \theta_j -c_j)+\lambda \frac{|\theta_j|}{\partial \theta_j} \ = a_j \theta_j-c_j+sign(\theta_j)\lambda,\ \theta=w</script><h2 id="Q3"><a href="#Q3" class="headerlink" title="Q3"></a>Q3</h2><p>if $\theta_j \gt 0$, then $d(\theta_j)=a_j\theta_j-c_j+\lambda$. If f is convex then $d_(\theta_j)=0$ can minimize f, so that $argmin_{\theta_j}d(\theta_j)=\frac{c_j-\lambda}{a_j}=-\frac{1}{a_j}(\lambda-c_j)$<br>condition of $\theta_j \lt 0$ is the same : $d(\theta_j)=\frac{1}{a_j}(\lambda+c_j)$ </p>
<h2 id="Q4"><a href="#Q4" class="headerlink" title="Q4"></a>Q4</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="/about/">Xiao L.</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab2%20Lasso%20and%20Coordinate%20Descent/">http://example.com/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab2%20Lasso%20and%20Coordinate%20Descent/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Note/">Note</a><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning</a></div><div class="post_share"><div class="social-share" data-image="/img/3.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab1%20Linear%20Descent,%20SGD,%20and%20Regularization/"><img class="prev-cover" src="/img/2.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/10/Course%20Note/SCU/Undergrad%20Operating%20System%20Overview/"><img class="next-cover" src="/img/74.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Undergrad Operating System Overview</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab1%20Linear%20Descent,%20SGD,%20and%20Regularization/" title="DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization"><img class="cover" src="/img/2.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-10</div><div class="title">DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization</div></div></a></div><div><a href="/2022/12/10/Course%20Note/Gatech/ECE%206607%20Computer%20Communication%20Network/" title="ECE 6607 Computer Communication Network"><img class="cover" src="/img/1.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-10</div><div class="title">ECE 6607 Computer Communication Network</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xiao L.</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">82</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/luoxiao23333"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/luoxiao23333" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:lxiao70@gatech.edu" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/www.linkedin.com/in/xiao-l-0489191a4" target="_blank" title="Linkedin"><i class="fab fa-linkedin"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_42971794" target="_blank" title="CSDN"><i class="fas fa-blog"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Import-and-Construct-Dataset"><span class="toc-number">1.</span> <span class="toc-text">Import and Construct Dataset</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Ridge-Regression"><span class="toc-number">2.</span> <span class="toc-text">Ridge Regression</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Coordinate-Descent-for-Lasso-Regression"><span class="toc-number">3.</span> <span class="toc-text">Coordinate Descent for Lasso Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Compute-square-loss"><span class="toc-number">3.1.</span> <span class="toc-text">Compute square loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Find-min-loss"><span class="toc-number">3.2.</span> <span class="toc-text">Find min loss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Find-how-many-true-zero"><span class="toc-number">3.2.1.</span> <span class="toc-text">Find how many true zero</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Homotopy-Method"><span class="toc-number">3.3.</span> <span class="toc-text">Homotopy Method</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#compare-time-between-warm-start-and-non-warm-start"><span class="toc-number">3.4.</span> <span class="toc-text">compare time between warm start and non warm start</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vectorize-coordinate-descent"><span class="toc-number">3.5.</span> <span class="toc-text">vectorize coordinate descent</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Derive-Coordinate-Descent"><span class="toc-number">4.</span> <span class="toc-text">Derive Coordinate Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Q1"><span class="toc-number">4.1.</span> <span class="toc-text">Q1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q2"><span class="toc-number">4.2.</span> <span class="toc-text">Q2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q3"><span class="toc-number">4.3.</span> <span class="toc-text">Q3</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q4"><span class="toc-number">4.4.</span> <span class="toc-text">Q4</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab1%20Linear%20Descent,%20SGD,%20and%20Regularization/" title="DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization"><img src="/img/2.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization"/></a><div class="content"><a class="title" href="/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab1%20Linear%20Descent,%20SGD,%20and%20Regularization/" title="DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization">DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization</a><time datetime="2022-12-10T13:03:25.000Z" title="Created 2022-12-10 08:03:25">2022-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/10/Algorithm/A%20Research%20Problem%EF%BC%88%E6%AC%A7%E6%8B%89%E5%87%BD%E6%95%B0%EF%BC%8CDFS%EF%BC%89/" title="A Research Problem（欧拉函数，DFS）"><img src="/img/4.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="A Research Problem（欧拉函数，DFS）"/></a><div class="content"><a class="title" href="/2022/12/10/Algorithm/A%20Research%20Problem%EF%BC%88%E6%AC%A7%E6%8B%89%E5%87%BD%E6%95%B0%EF%BC%8CDFS%EF%BC%89/" title="A Research Problem（欧拉函数，DFS）">A Research Problem（欧拉函数，DFS）</a><time datetime="2022-12-10T13:03:25.000Z" title="Created 2022-12-10 08:03:25">2022-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/10/Algorithm/A%20Spy%20in%20the%20Metro/" title="A Spy in the Metro"><img src="/img/5.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="A Spy in the Metro"/></a><div class="content"><a class="title" href="/2022/12/10/Algorithm/A%20Spy%20in%20the%20Metro/" title="A Spy in the Metro">A Spy in the Metro</a><time datetime="2022-12-10T13:03:25.000Z" title="Created 2022-12-10 08:03:25">2022-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/10/Algorithm/Almost%20Prime%20Numbers%EF%BC%88%E7%B4%A0%E6%95%B0%E7%AD%9B%EF%BC%89/" title="Almost Prime Numbers（素数筛）"><img src="/img/6.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Almost Prime Numbers（素数筛）"/></a><div class="content"><a class="title" href="/2022/12/10/Algorithm/Almost%20Prime%20Numbers%EF%BC%88%E7%B4%A0%E6%95%B0%E7%AD%9B%EF%BC%89/" title="Almost Prime Numbers（素数筛）">Almost Prime Numbers（素数筛）</a><time datetime="2022-12-10T13:03:25.000Z" title="Created 2022-12-10 08:03:25">2022-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/10/Algorithm/And%20Then%20There%20Was%20One/" title="And Then There Was One"><img src="/img/7.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="And Then There Was One"/></a><div class="content"><a class="title" href="/2022/12/10/Algorithm/And%20Then%20There%20Was%20One/" title="And Then There Was One">And Then There Was One</a><time datetime="2022-12-10T13:03:25.000Z" title="Created 2022-12-10 08:03:25">2022-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Xiao L.</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>