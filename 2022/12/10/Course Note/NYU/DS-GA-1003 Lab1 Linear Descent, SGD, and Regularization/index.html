<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization | Always on the way</title><meta name="author" content="Xiao Luo.,luo.1632@osu.edu"><meta name="copyright" content="Xiao Luo."><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="This is my lab from DS-GA-1003 NYUI put all my code in my github Lab1 Gradient DescentImport and Init Datasetcompile feature normalization first 123456# Importimport matplotlib.pyplot as pltimport num">
<meta property="og:type" content="article">
<meta property="og:title" content="DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization">
<meta property="og:url" content="http://example.com/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab1%20Linear%20Descent,%20SGD,%20and%20Regularization/index.html">
<meta property="og:site_name" content="Always on the way">
<meta property="og:description" content="This is my lab from DS-GA-1003 NYUI put all my code in my github Lab1 Gradient DescentImport and Init Datasetcompile feature normalization first 123456# Importimport matplotlib.pyplot as pltimport num">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/2.jpeg">
<meta property="article:published_time" content="2022-12-11T02:03:25.000Z">
<meta property="article:modified_time" content="2025-12-28T03:34:06.383Z">
<meta property="article:author" content="Xiao Luo.">
<meta property="article:tag" content="Note">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/2.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab1%20Linear%20Descent,%20SGD,%20and%20Regularization/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-12-27 22:34:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">82</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Algorithmic Problem Solving</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/2.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Always on the way</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Algorithmic Problem Solving</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-12-11T02:03:25.000Z" title="Created 2022-12-10 21:03:25">2022-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-12-28T03:34:06.383Z" title="Updated 2025-12-27 22:34:06">2025-12-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Course-Note/">Course Note</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Course-Note/DS-DA-1003-Machine-Learning/">DS-DA-1003 Machine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>This is my lab from <a target="_blank" rel="noopener" href="https://davidrosenberg.github.io/ml2016/#lectures">DS-GA-1003 NYU</a><br>I put all my code in my <a target="_blank" rel="noopener" href="https://github.com/luoxiao23333/DS-GA-1003">github</a></p>
<h1 id="Lab1-Gradient-Descent"><a href="#Lab1-Gradient-Descent" class="headerlink" title="Lab1 Gradient Descent"></a>Lab1 Gradient Descent</h1><h1 id="Import-and-Init-Dataset"><a href="#Import-and-Init-Dataset" class="headerlink" title="Import and Init Dataset"></a>Import and Init Dataset</h1><p>compile feature normalization first</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Init data</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;hw1-data.csv&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">X = df.values[:, :-<span class="number">1</span>]</span><br><span class="line">y = df.values[:, -<span class="number">1</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">100</span>, random_state=<span class="number">10</span>)</span><br><span class="line">X_train, X_test = feature_normalization(X_train, X_test)</span><br><span class="line">X_train = np.hstack((X_train, np.ones((X_train.shape[<span class="number">0</span>], <span class="number">1</span>))))  <span class="comment"># Add bias term</span></span><br><span class="line">X_test = np.hstack((X_test, np.ones((X_test.shape[<span class="number">0</span>], <span class="number">1</span>))))  <span class="comment"># Add bias term</span></span><br></pre></td></tr></table></figure>
<h1 id="Feature-Normalization"><a href="#Feature-Normalization" class="headerlink" title="Feature Normalization"></a>Feature Normalization</h1><p>only operate train dataset.<br>shift all data to ensure they are all larger than 0, then rescale them into [0,1]  </p>
<script type="math/tex; mode=display">X_{norm}=\frac{X-X{min}}{X_{max}-X_{min}}</script><h2 id="Wanrning"><a href="#Wanrning" class="headerlink" title="Wanrning"></a>Wanrning</h2><p>数据标准化必须以特征列为单位。不能所有数据共同标准化。因为不同feature的数据尺度不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">feature_normalization</span>(<span class="params">train: np.ndarray, test: np.ndarray</span>) -&gt; [np.ndarray, np.ndarray]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Rescale the data so that each feature in the training set is in</span></span><br><span class="line"><span class="string">    the interval [0,1], and apply the same transformations to the test</span></span><br><span class="line"><span class="string">    set, using the statistics computed on the training set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        train - training set, a 2D numpy array of size (num_instances, num_features)</span></span><br><span class="line"><span class="string">        test  - test set, a 2D numpy array of size (num_instances, num_features)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        train_normalized - training set after normalization</span></span><br><span class="line"><span class="string">        test_normalized  - test set after normalization</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    min_value: np.ndarray = np.<span class="built_in">min</span>(train, axis=<span class="number">0</span>)</span><br><span class="line">    max_value: np.ndarray = np.<span class="built_in">max</span>(train, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(max_value)):</span><br><span class="line">        <span class="keyword">if</span> min_value[index] == max_value[index]:</span><br><span class="line">            min_value[index] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    train_normalized = (train - min_value) / (max_value - min_value + <span class="number">0.0</span>)</span><br><span class="line">    test_normalized = (test - min_value) / (max_value - min_value + <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_normalized, test_normalized</span><br></pre></td></tr></table></figure>
<h1 id="Linear-Regression-with-Full-Batch"><a href="#Linear-Regression-with-Full-Batch" class="headerlink" title="Linear Regression with Full Batch"></a>Linear Regression with Full Batch</h1><h2 id="Compute-Square-Loss"><a href="#Compute-Square-Loss" class="headerlink" title="Compute Square Loss"></a>Compute Square Loss</h2><p>Given a set of X, y, theta, compute the square loss for predicting y with X*theta<br>loss func: $J(\theta)=\frac{1}{2m}\sum<em>{i=1}^{m}(h</em>{\theta}(x<em>i)-y_i)^2$<br>In matrix formulation, $h</em>{\theta}(x_i)=\theta ^{T}x_i$<br>So $J(\theta)=\frac{1}{2m}(X\theta^{T}-y)(X\theta^{T}-y)^{T}$  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_square_loss</span>(<span class="params">X: np.ndarray, y: np.ndarray, theta: np.ndarray</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Given a set of X, y, theta, compute the square loss for predicting y with X*theta</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X - the feature vector, 2D numpy array of size (num_instances, num_features)</span></span><br><span class="line"><span class="string">        y - the label vector, 1D numpy array of size (num_instances)</span></span><br><span class="line"><span class="string">        theta - the parameter vector, 1D array of size (num_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        loss - the square loss, scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    num_instances = X.shape[<span class="number">0</span>]</span><br><span class="line">    auction = np.dot(X, theta)</span><br><span class="line">    difference: np.ndarray = auction - y</span><br><span class="line"></span><br><span class="line">    loss: <span class="built_in">float</span> = <span class="number">0.5</span> / num_instances * np.dot(difference, difference)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="Compute-Square-Loss-Gradient"><a href="#Compute-Square-Loss-Gradient" class="headerlink" title="Compute Square Loss Gradient"></a>Compute Square Loss Gradient</h2><p>$J(\theta)=\frac{1}{2m}(X\theta^{T}-y)(X\theta^{T}-y)^{T}$<br>$\bigtriangledown\theta J(\theta)=\frac{1}{m}(X\theta^{T}-y)X$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_square_loss_gradient</span>(<span class="params">X: np.ndarray, y: np.ndarray, theta: np.ndarray</span>) -&gt; np.ndarray:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute gradient of the square loss (as defined in compute_square_loss), at the point theta.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X - the feature vector, 2D numpy array of size (num_instances, num_features)</span></span><br><span class="line"><span class="string">        y - the label vector, 1D numpy array of size (num_instances)</span></span><br><span class="line"><span class="string">        theta - the parameter vector, 1D numpy array of size (num_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        grad - gradient vector, 1D numpy array of size (num_features)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    num_instances = X.shape[<span class="number">0</span>]</span><br><span class="line">    difference: np.ndarray = np.dot(X, theta) - y</span><br><span class="line">    grad = <span class="number">1.0</span> / (num_instances+<span class="number">0.0</span>) * np.dot(difference.T, X)</span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<h2 id="Gradient-Check"><a href="#Gradient-Check" class="headerlink" title="Gradient Check"></a>Gradient Check</h2><p>Check that the function compute_square_loss_gradient returns the correct gradient for the given X, y, and theta.<br>For all directions: let $e_1 = (1,0,0,\dots,0), e_2 = (0,1,0,\dots,0), \dots, e_d = (0,\dots,0,1)$<br>Use approximation: <script type="math/tex">\frac{J(\theta + epsilon * e_i) - J(\theta - epsilon * e_i)}{(2*epsilon)}</script> for each direction<br>Then for each direction of gradient, check if Euclidean distance of approximation and computed is exceed tolerance(1e-4 default)  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">grad_checker</span>(<span class="params">X: np.ndarray, y: np.ndarray, theta: np.ndarray, epsilon=<span class="number">0.01</span>, tolerance=<span class="number">1e-4</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Implement Gradient Checker</span></span><br><span class="line"><span class="string">    Check that the function compute_square_loss_gradient returns the</span></span><br><span class="line"><span class="string">    correct gradient for the given X, y, and theta.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Let d be the number of features. Here we numerically estimate the</span></span><br><span class="line"><span class="string">    gradient by approximating the directional derivative in each of</span></span><br><span class="line"><span class="string">    the d coordinate directions:</span></span><br><span class="line"><span class="string">    (e_1 = (1,0,0,...,0), e_2 = (0,1,0,...,0), ..., e_d = (0,...,0,1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The approximation for the directional derivative of J at the point</span></span><br><span class="line"><span class="string">    theta in the direction e_i is given by:</span></span><br><span class="line"><span class="string">    ( J(theta + epsilon * e_i) - J(theta - epsilon * e_i) ) / (2*epsilon).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    We then look at the Euclidean distance between the gradient</span></span><br><span class="line"><span class="string">    computed using this approximation and the gradient computed by</span></span><br><span class="line"><span class="string">    compute_square_loss_gradient(X, y, theta).  If the Euclidean</span></span><br><span class="line"><span class="string">    distance exceeds tolerance, we say the gradient is incorrect.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X - the feature vector, 2D numpy array of size (num_instances, num_features)</span></span><br><span class="line"><span class="string">        y - the label vector, 1D numpy array of size (num_instances)</span></span><br><span class="line"><span class="string">        theta - the parameter vector, 1D numpy array of size (num_features)</span></span><br><span class="line"><span class="string">        epsilon - the epsilon used in approximation</span></span><br><span class="line"><span class="string">        tolerance - the tolerance error</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        A boolean value indicate whether the gradient is correct or not</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    true_gradient: np.ndarray = compute_square_loss_gradient(X, y, theta)  <span class="comment"># the true gradient</span></span><br><span class="line">    num_features = theta.shape[<span class="number">0</span>]</span><br><span class="line">    approx_grad: np.ndarray = np.zeros(num_features)  <span class="comment"># Initialize the gradient we approximate</span></span><br><span class="line">    <span class="comment"># TODO</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(num_features):</span><br><span class="line">        e_i = np.zeros(num_features)</span><br><span class="line">        e_i[index] = <span class="number">1</span></span><br><span class="line">        theta_plus = theta + epsilon * e_i</span><br><span class="line">        theta_minus = theta - epsilon * e_i</span><br><span class="line">        approx_grad[index] = \</span><br><span class="line">            (compute_square_loss(X, y, theta_plus) - compute_square_loss(X, y, theta_minus)) \</span><br><span class="line">            / (<span class="number">2</span> * epsilon)</span><br><span class="line">    distance = np.linalg.norm(approx_grad-true_gradient)</span><br><span class="line">    <span class="keyword">return</span> distance &lt; tolerance</span><br></pre></td></tr></table></figure>
<h2 id="Generic-Gradient-Checker"><a href="#Generic-Gradient-Checker" class="headerlink" title="Generic Gradient Checker"></a>Generic Gradient Checker</h2><p>Similar as Gradient Checker<br>The functions takes objective_func and gradient_func as parameters. And check whether gradient_func(X, y, theta) returned<br>the true gradient for objective_func(X, y, theta).<br>Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generic_gradient_checker</span>(<span class="params">X, y, theta, objective_func, gradient_func, epsilon=<span class="number">0.01</span>, tolerance=<span class="number">1e-4</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The functions takes objective_func and gradient_func as parameters. And check whether gradient_func(X, y, theta) returned</span></span><br><span class="line"><span class="string">    the true gradient for objective_func(X, y, theta).</span></span><br><span class="line"><span class="string">    Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    true_gradient: np.ndarray = gradient_func(X, y, theta)  <span class="comment"># the true gradient</span></span><br><span class="line">    num_features = theta.shape[<span class="number">0</span>]</span><br><span class="line">    approx_grad: np.ndarray = np.zeros(num_features)  <span class="comment"># Initialize the gradient we approximate</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(num_features):</span><br><span class="line">        e_i = np.zeros(num_features)</span><br><span class="line">        e_i[index] = <span class="number">1</span></span><br><span class="line">        theta_plus = theta + epsilon * e_i</span><br><span class="line">        theta_minus = theta - epsilon * e_i</span><br><span class="line">        approx_grad[index] = \</span><br><span class="line">            (objective_func(X, y, theta_plus) - objective_func(X, y, theta_minus)) \</span><br><span class="line">            / (<span class="number">2</span> * epsilon)</span><br><span class="line">    distance = np.linalg.norm(approx_grad - true_gradient)</span><br><span class="line">    <span class="keyword">return</span> distance &lt; tolerance</span><br></pre></td></tr></table></figure>
<h2 id="Batch-Grad-Descent"><a href="#Batch-Grad-Descent" class="headerlink" title="Batch Grad Descent"></a>Batch Grad Descent</h2><p>每次迭代计算梯度，然后$\theta$下降一次梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batch_grad_descent</span>(<span class="params">X: np.ndarray, y: np.ndarray, step_size=<span class="number">0.1</span>, num_iter=<span class="number">1000</span>, check_gradient=<span class="literal">False</span></span>) \</span><br><span class="line">        -&gt; [np.ndarray, np.ndarray]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    In this question you will implement batch gradient descent to</span></span><br><span class="line"><span class="string">    minimize the square loss objective</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X - the feature vector, 2D numpy array of size (num_instances, num_features)</span></span><br><span class="line"><span class="string">        y - the label vector, 1D numpy array of size (num_instances)</span></span><br><span class="line"><span class="string">        step - step size in gradient descent</span></span><br><span class="line"><span class="string">        num_iter - number of iterations to run</span></span><br><span class="line"><span class="string">        check_gradient - a boolean value indicating whether checking the gradient when updating</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        theta_hist - store the history of parameter vector in iteration, 2D numpy array of size (num_iter+1, num_features)</span></span><br><span class="line"><span class="string">                    for instance, theta in iteration 0 should be theta_hist[0], theta in iteration (num_iter) is theta_hist[-1]</span></span><br><span class="line"><span class="string">        loss_hist - the history of objective function vector, 1D numpy array of size (num_iter+1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_instances, num_features = X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>]</span><br><span class="line">    theta_hist = np.zeros((num_iter + <span class="number">1</span>, num_features))  <span class="comment"># Initialize theta_hist</span></span><br><span class="line">    loss_hist = np.zeros(num_iter + <span class="number">1</span>)  <span class="comment"># initialize loss_hist</span></span><br><span class="line">    theta = np.ones(num_features)  <span class="comment"># initialize theta</span></span><br><span class="line">    <span class="comment"># TODO</span></span><br><span class="line">    theta_hist[<span class="number">0</span>] = theta</span><br><span class="line">    loss_hist[<span class="number">0</span>] = compute_square_loss(X, y, theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_iter + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> check_gradient:</span><br><span class="line">            <span class="keyword">assert</span> (grad_checker(X, y, theta) <span class="keyword">is</span> <span class="literal">True</span>)</span><br><span class="line">        grad = compute_square_loss_gradient(X, y, theta)</span><br><span class="line">        theta = theta - step_size*grad.transpose()</span><br><span class="line">        theta_hist[iteration] = theta</span><br><span class="line">        loss = compute_square_loss(X, y, theta.transpose())</span><br><span class="line">        loss_hist[iteration] = loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta_hist, loss_hist</span><br></pre></td></tr></table></figure>
<h2 id="Draw-Convergence-Diagram"><a href="#Draw-Convergence-Diagram" class="headerlink" title="Draw Convergence Diagram"></a>Draw Convergence Diagram</h2><p>Try step sizes [0.01, 0.05, 0.1, 0.101, 0.2]. Plot the value of the objective function as a function of the number of steps for each step sizes. Briefly summarize your findings.</p>
<h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><p>If step size is too small, it will converge will slow. If step size is too large, it will oscillate and therefore, never converge.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">draw</span>(<span class="params">theta_hist, loss_hist, step_size, num_iter</span>):</span><br><span class="line">    x_index = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iter + <span class="number">1</span>):</span><br><span class="line">        x_index.append(i)</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Iterations&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Loss over Step Size&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> loss_hist[-<span class="number">1</span>] &gt; <span class="number">1000</span>:</span><br><span class="line">        plt.yscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">    plt.plot(x_index, loss_hist, label=step_size)</span><br><span class="line">    plt.legend()</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    plt.plot(x_index, theta_hist.transpose()[<span class="number">0</span>], label=step_size)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Iterations&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Theta&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Theta over Step Size&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> theta_hist.transpose()[<span class="number">0</span>][-<span class="number">1</span>] &gt; <span class="number">100</span>:</span><br><span class="line">        plt.yscale(<span class="string">&#x27;symlog&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">converge_test</span>(<span class="params">X, y, num_iter=<span class="number">100</span></span>):</span><br><span class="line">    step_sizes = np.array([<span class="number">0.0001</span>, <span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.101</span>, <span class="number">0.2</span>])</span><br><span class="line">    x_index = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iter + <span class="number">1</span>):</span><br><span class="line">        x_index.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes:</span><br><span class="line">        theta_hist, loss_hist = batch_grad_descent(X, y,</span><br><span class="line">                                                   step_size=step_size,</span><br><span class="line">                                                   num_iter=num_iter,</span><br><span class="line">                                                   check_gradient=<span class="literal">False</span>)</span><br><span class="line">        draw(theta_hist, loss_hist, step_size, num_iter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">converge_test(X_train, y_train)</span><br></pre></td></tr></table></figure>
<p><img src="output_18_0.png" alt="png"></p>
<p><img src="output_18_1.png" alt="png"></p>
<p><img src="output_18_2.png" alt="png"></p>
<p><img src="output_18_3.png" alt="png"></p>
<p><img src="output_18_4.png" alt="png"></p>
<p><img src="output_18_5.png" alt="png"></p>
<h1 id="Ridge-Regression-with-L2-Regularized"><a href="#Ridge-Regression-with-L2-Regularized" class="headerlink" title="Ridge Regression with L2 Regularized"></a>Ridge Regression with L2 Regularized</h1><h2 id="Compute-Regularized-Square-Loss-Gradient"><a href="#Compute-Regularized-Square-Loss-Gradient" class="headerlink" title="Compute Regularized Square Loss Gradient"></a>Compute Regularized Square Loss Gradient</h2><p>In Ridge Regression<br>Given a set of X, y, theta, compute the square loss for predicting y with X*theta<br>loss func: $J(\theta)=\frac{1}{2m}\sum<em>{i=1}^{m}(h</em>{\theta}(x<em>i)-y_i)^2+\lambda \theta^{T}\theta$<br>In matrix formulation, $h</em>{\theta}(x_i)=\theta ^{T}x_i$<br>So $J(\theta)=\frac{1}{2m}(X\theta^{T}-y)(X\theta^{T}-y)^{T}+\lambda \theta^{T}\theta$<br>$\bigtriangledown J(\theta)=\frac{1}{m}(x\theta -y)^{T}x+2\lambda \theta^{T}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_regularized_square_loss_gradient</span>(<span class="params">X, y, theta, lambda_reg</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the gradient of L2-regularized square loss function given X, y and theta</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X - the feature vector, 2D numpy array of size (num_instances, num_features)</span></span><br><span class="line"><span class="string">        y - the label vector, 1D numpy array of size (num_instances)</span></span><br><span class="line"><span class="string">        theta - the parameter vector, 1D numpy array of size (num_features)</span></span><br><span class="line"><span class="string">        lambda_reg - the regularization coefficient</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        grad - gradient vector, 1D numpy array of size (num_features)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># TODO</span></span><br><span class="line">    square_loss_gradient = compute_square_loss_gradient(X, y, theta)</span><br><span class="line">    <span class="keyword">return</span> square_loss_gradient + <span class="number">2</span>*lambda_reg*theta.T</span><br></pre></td></tr></table></figure>
<h2 id="Regularized-Grad-Descent"><a href="#Regularized-Grad-Descent" class="headerlink" title="Regularized Grad Descent"></a>Regularized Grad Descent</h2><p>same as line regression, except use regularized squre loss function</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">regularized_grad_descent</span>(<span class="params">X, y, step_size=<span class="number">0.05</span>, lambda_reg=<span class="number">1</span>, num_iter=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X - the feature vector, 2D numpy array of size (num_instances, num_features)</span></span><br><span class="line"><span class="string">        y - the label vector, 1D numpy array of size (num_instances)</span></span><br><span class="line"><span class="string">        alpha - step size in gradient descent</span></span><br><span class="line"><span class="string">        lambda_reg - the regularization coefficient</span></span><br><span class="line"><span class="string">        numIter - number of iterations to run</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        theta_hist - the history of parameter vector, 2D numpy array of size (num_iter+1, num_features)</span></span><br><span class="line"><span class="string">        loss_hist - the history of regularized loss value, 1D numpy array</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_instances, num_features = X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>]</span><br><span class="line">    theta_hist = np.zeros((num_iter + <span class="number">1</span>, num_features))  <span class="comment"># Initialize theta_hist</span></span><br><span class="line">    loss_hist = np.zeros(num_iter + <span class="number">1</span>)  <span class="comment"># initialize loss_hist</span></span><br><span class="line">    theta = np.ones(num_features)  <span class="comment"># initialize theta</span></span><br><span class="line">    theta_hist[<span class="number">0</span>] = theta</span><br><span class="line">    loss_hist[<span class="number">0</span>] = compute_square_loss(X, y, theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_iter + <span class="number">1</span>):</span><br><span class="line">        grad = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)</span><br><span class="line">        theta = theta - step_size * grad.T</span><br><span class="line">        theta_hist[iteration] = theta</span><br><span class="line">        loss_hist[iteration] = compute_square_loss(X, y, theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta_hist, loss_hist</span><br></pre></td></tr></table></figure>
<h2 id="Visualize-Ridge-Regression"><a href="#Visualize-Ridge-Regression" class="headerlink" title="Visualize Ridge Regression"></a>Visualize Ridge Regression</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_regularized_batch_gradient_descent</span>(<span class="params">X, y</span>):</span><br><span class="line">    num_iter = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> lambda_reg <span class="keyword">in</span> [<span class="number">1e-5</span>, <span class="number">1e-3</span>, <span class="number">1e-1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]:</span><br><span class="line">        theta_hist, loss_hist = regularized_grad_descent(X, y, lambda_reg = lambda_reg, num_iter=num_iter)</span><br><span class="line">        draw(theta_hist, loss_hist, lambda_reg, num_iter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">visualize_regularized_batch_gradient_descent(X_train, y_train)</span><br></pre></td></tr></table></figure>
<p><img src="output_25_0.png" alt="png"></p>
<p><img src="output_25_1.png" alt="png"></p>
<p><img src="output_25_2.png" alt="png"></p>
<p><img src="output_25_3.png" alt="png"></p>
<p><img src="output_25_4.png" alt="png"></p>
<p><img src="output_25_5.png" alt="png"></p>
<h1 id="Stochastic-Grad-Descent-SGD"><a href="#Stochastic-Grad-Descent-SGD" class="headerlink" title="Stochastic Grad Descent(SGD)"></a>Stochastic Grad Descent(SGD)</h1><p>If dataset is too large, compute loss gradient for the whole dataset is time-comsuming.<br>So we can divide a dataset into many batch, i.e. minibatch<br><strong>Epoch</strong>: if you run some minibatch that coverge the dataset, then you run 1 epoch<br>The batch size of the code is 50% of whole dataset.<br>The epoch is 100  </p>
<h2 id="4-step-size-methods"><a href="#4-step-size-methods" class="headerlink" title="4 step size methods:"></a>4 step size methods:</h2><p>if step<em>size is a float, then the step size in every iteration is constant.<br>if step_size == “1/sqrt(t)”, $step\ size = \frac{1}{\sqrt{t}}$<br>if step_size == “1/t”, $StepSize= \frac{1}{t}$<br>if step_size == “frac”, $$StepSize=\frac{StepSize</em>{0}}{1+StepSize<em>{0}<em>lambda</em>t}$$, whereas $StepSize</em>{0}$ is set up by you</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">stochastic_grad_descent</span>(<span class="params">X, y, step_size=<span class="number">0.1</span>, lambda_reg: <span class="built_in">float</span> = <span class="number">1</span>, num_iter=<span class="number">100</span></span>) -&gt; [np.ndarray, np.ndarray]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    In this question you will implement stochastic gradient descent with a regularization term</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        :param X - the feature vector, 2D numpy array of size (num_instances, num_features)</span></span><br><span class="line"><span class="string">        :param y - the label vector, 1D numpy array of size (num_instances)</span></span><br><span class="line"><span class="string">        :param step_size - string or float. step size in gradient descent</span></span><br><span class="line"><span class="string">                NOTE: In SGD, it&#x27;s not always a good idea to use a fixed step size. Usually it&#x27;s set to 1/sqrt(t) or 1/t</span></span><br><span class="line"><span class="string">                if step_size is a float, then the step size in every iteration is alpha.</span></span><br><span class="line"><span class="string">                if step_size == &quot;1/sqrt(t)&quot;, alpha = 1/sqrt(t)</span></span><br><span class="line"><span class="string">                if step_size == &quot;1/t&quot;, alpha = 1/t</span></span><br><span class="line"><span class="string">                if step_size == &quot;frac&quot;, step_size = step_size_0/(1+step_size_0*lambda*t)&quot;</span></span><br><span class="line"><span class="string">        :param lambda_reg - the regularization coefficient</span></span><br><span class="line"><span class="string">        :param num_iter - number of epochs (i.e. number of times) to go through the whole training set</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        :return theta_hist - the history of parameter vector, 3D numpy array of size (num_iter, num_instances, num_features)</span></span><br><span class="line"><span class="string">        :return loss hist - the history of regularized loss function vector, 2D numpy array of size(num_iter, num_instances)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_instances, num_features = X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>]</span><br><span class="line">    theta = np.ones(num_features)  <span class="comment"># Initialize theta</span></span><br><span class="line"></span><br><span class="line">    theta_hist = np.zeros((num_iter + <span class="number">1</span>, num_features))  <span class="comment"># Initialize theta_hist</span></span><br><span class="line">    loss_hist = np.zeros(num_iter + <span class="number">1</span>)  <span class="comment"># Initialize loss_hist</span></span><br><span class="line">    theta_hist[<span class="number">0</span>] = theta</span><br><span class="line">    loss_hist[<span class="number">0</span>] = compute_square_loss(X, y, theta)</span><br><span class="line">    step_size_0: <span class="built_in">float</span> = <span class="number">1.0</span></span><br><span class="line">    step_size_method = step_size</span><br><span class="line">    <span class="keyword">if</span> step_size_method == <span class="string">&quot;frac&quot;</span>:</span><br><span class="line">        step_size_0: <span class="built_in">float</span> = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_iter + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> step_size_method == <span class="string">&quot;1/sqrt(t)&quot;</span>:</span><br><span class="line">            step_size = <span class="number">1.0</span> / np.sqrt(t + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> step_size_method == <span class="string">&quot;1/t&quot;</span>:</span><br><span class="line">            step_size = <span class="number">1</span> / (t + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> step_size_method == <span class="string">&quot;frac&quot;</span>:</span><br><span class="line">            step_size = step_size_0 / (<span class="number">1.0</span> + step_size_0 * lambda_reg * (t + <span class="number">1.0</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        X_1, X_2, y_1, y_2 = train_test_split(X, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">10</span>)</span><br><span class="line">        X_1, X_2 = feature_normalization(X_1, X_2)</span><br><span class="line">        grad = compute_regularized_square_loss_gradient(X_1, y_1, theta, lambda_reg)</span><br><span class="line">        theta = theta - step_size * grad.T</span><br><span class="line">        grad = compute_regularized_square_loss_gradient(X_2, y_2, theta, lambda_reg)</span><br><span class="line">        theta = theta - step_size * grad.T</span><br><span class="line">        loss_hist[t] = compute_square_loss(X, y, theta) + np.dot(theta, theta) * lambda_reg</span><br><span class="line">        theta_hist[t] = theta</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta_hist, loss_hist</span><br></pre></td></tr></table></figure>
<h2 id="Visualize-SGD-by-4-methods"><a href="#Visualize-SGD-by-4-methods" class="headerlink" title="Visualize SGD by 4 methods"></a>Visualize SGD by 4 methods</h2><p>For “0.05”, it need you to choose this manually.<br>For “1/sqrt(1)” and “1/t”, they are diverged!<br>For “frac”, it’s oscillated but converged finally!<br>The reason why “1/sqrt(1)” and “1/t” are diverged is that: they have too large step size initially that let them run out of the curve. And their step sizes decrease too fast that they can not turn back to the minimum point. And frac mode solve this by decreasing slowly.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_sgd</span>(<span class="params">X, y</span>):</span><br><span class="line">    num_iter = <span class="number">1000</span></span><br><span class="line">    <span class="keyword">for</span> step_size <span class="keyword">in</span> [<span class="number">0.05</span>, <span class="string">&quot;1/sqrt(t)&quot;</span>, <span class="string">&quot;1/t&quot;</span>, <span class="string">&quot;frac&quot;</span>]:</span><br><span class="line">        theta_hist, loss_hist = stochastic_grad_descent(X, y, step_size,</span><br><span class="line">                                                        lambda_reg=<span class="number">1e-5</span>, num_iter=num_iter)</span><br><span class="line">        draw(theta_hist, loss_hist, step_size, num_iter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">visualize_sgd(X_train, y_train)</span><br></pre></td></tr></table></figure>
<p><img src="output_29_0.png" alt="png"></p>
<p><img src="output_29_1.png" alt="png"></p>
<p><img src="output_29_2.png" alt="png"></p>
<p><img src="output_29_3.png" alt="png"></p>
<h1 id="Compare-Full-Batch-and-SGD"><a href="#Compare-Full-Batch-and-SGD" class="headerlink" title="Compare Full Batch and SGD"></a>Compare Full Batch and SGD</h1><p>SGD converge a little quickly with comparing of iteration times. But Full Batch can adapt more large step size.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_compare_batch_and_sgd</span>(<span class="params">X, y</span>):</span><br><span class="line">    num_iter = <span class="number">100</span></span><br><span class="line">    <span class="keyword">for</span> step_size <span class="keyword">in</span> [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]:</span><br><span class="line">        theta_hist_batch, loss_hit_batch = batch_grad_descent(X, y, step_size, num_iter)</span><br><span class="line">        theta_hist_sgd, loss_hit_sgd = stochastic_grad_descent(X, y,</span><br><span class="line">                                                               step_size=step_size,</span><br><span class="line">                                                               num_iter=num_iter,</span><br><span class="line">                                                               lambda_reg=<span class="number">1e-4</span>)</span><br><span class="line">        plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;Iterations&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">&#x27;Loss over Step Size = &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(step_size))</span><br><span class="line">        plt.plot(loss_hit_batch, label=<span class="string">&quot;Full Batch&quot;</span>)</span><br><span class="line">        plt.plot(loss_hit_sgd, label=<span class="string">&quot;SGD&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> loss_hit_batch[-<span class="number">1</span>] &gt; <span class="number">1000</span>:</span><br><span class="line">            plt.yscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">        plt.legend()</span><br><span class="line"></span><br><span class="line">        plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;Iterations&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Theta&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">&#x27;Theta over Step Size = &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(step_size))</span><br><span class="line">        plt.plot(theta_hist_batch.transpose()[<span class="number">0</span>], label=<span class="string">&quot;Full Batch&quot;</span>)</span><br><span class="line">        plt.plot(theta_hist_sgd.transpose()[<span class="number">0</span>], label=<span class="string">&quot;SGD&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> loss_hit_batch[-<span class="number">1</span>] &gt; <span class="number">1000</span>:</span><br><span class="line">            plt.yscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">        plt.legend()</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">visualize_compare_batch_and_sgd(X_train, y_train)</span><br></pre></td></tr></table></figure>
<p><img src="output_31_0.png" alt="png"></p>
<p><img src="output_31_1.png" alt="png"></p>
<p><img src="output_31_2.png" alt="png"></p>
<p><img src="output_31_3.png" alt="png"></p>
<p><img src="output_31_4.png" alt="png"></p>
<h1 id="Overfit-Analyse"><a href="#Overfit-Analyse" class="headerlink" title="Overfit Analyse"></a>Overfit Analyse</h1><p>Since you have different dataset for training and testing, if you apply for too large iterations when train, the model will just more only “remember” the train dataset(overfit). The model therefore are lack of generalization and get larger mean square error in test dataset<br>Therefore, we can add L2 regularization to solve this. But L2 will make trivial contribution if too small, or cause too large bias if too large.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">overfit_analyse</span>(<span class="params">X_train, y_train, X_test, y_test</span>):</span><br><span class="line">    num_iter = <span class="number">10000</span></span><br><span class="line">    <span class="keyword">for</span> step_size <span class="keyword">in</span> [<span class="number">0.05</span>, <span class="number">0.01</span>]:</span><br><span class="line">        theta_hist_non_reg, _ = \</span><br><span class="line">            batch_grad_descent(X_train, y_train, step_size=step_size, num_iter=num_iter)</span><br><span class="line"></span><br><span class="line">        loss_trace_non_reg: <span class="built_in">list</span> = \</span><br><span class="line">            [compute_square_loss(X_test, y_test, theta) <span class="keyword">for</span> theta <span class="keyword">in</span> theta_hist_non_reg]</span><br><span class="line">        min_index = np.argmin(loss_trace_non_reg)</span><br><span class="line">        plt.plot(loss_trace_non_reg[<span class="number">100</span>:-<span class="number">1</span>], label=<span class="string">&quot;No Regularization&quot;</span>)</span><br><span class="line">        show_max = <span class="string">&#x27;[&#x27;</span> + <span class="built_in">str</span>(min_index) + <span class="string">&#x27;,&#x27;</span> + <span class="built_in">str</span>(loss_trace_non_reg[min_index])[<span class="number">0</span>:<span class="number">4</span>] + <span class="string">&#x27;]&#x27;</span></span><br><span class="line">        plt.annotate(show_max, xytext=(min_index - <span class="number">100</span>, loss_trace_non_reg[min_index]),</span><br><span class="line">                     xy=(min_index - <span class="number">100</span>, loss_trace_non_reg[min_index]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> lambda_reg <span class="keyword">in</span> [<span class="number">1e-3</span>, <span class="number">1e-2</span>, <span class="number">0.1</span>]:</span><br><span class="line">            theta_hist_reg, _ = \</span><br><span class="line">                regularized_grad_descent(X_train, y_train,</span><br><span class="line">                                         lambda_reg=lambda_reg, step_size=step_size, num_iter=num_iter)</span><br><span class="line"></span><br><span class="line">            loss_trace_reg: <span class="built_in">list</span> = \</span><br><span class="line">                [compute_square_loss(X_test, y_test, theta)</span><br><span class="line">                 <span class="keyword">for</span> theta <span class="keyword">in</span> theta_hist_reg]</span><br><span class="line">            plt.plot(loss_trace_reg[<span class="number">100</span>:-<span class="number">1</span>], label=<span class="string">&quot;Reg Lambda &#123;0&#125;&quot;</span>.<span class="built_in">format</span>(lambda_reg))</span><br><span class="line">            min_index = np.argmin(loss_trace_reg)</span><br><span class="line">            show_max = <span class="string">&#x27;[&#x27;</span> + <span class="built_in">str</span>(min_index) + <span class="string">&#x27;,&#x27;</span> + <span class="built_in">str</span>(loss_trace_reg[min_index])[<span class="number">0</span>:<span class="number">4</span>] + <span class="string">&#x27;]&#x27;</span></span><br><span class="line">            plt.annotate(show_max, xytext=(min_index - <span class="number">100</span>, loss_trace_reg[min_index]),</span><br><span class="line">                         xy=(min_index - <span class="number">100</span>, loss_trace_reg[min_index]))</span><br><span class="line"></span><br><span class="line">        plt.legend()</span><br><span class="line">        plt.xlabel(<span class="string">&quot;Iterations&quot;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">        plt.title(<span class="string">&quot;Loss change over iterations with step size: &#123;0&#125;&quot;</span>.<span class="built_in">format</span>(step_size))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">overfit_analyse(X_train, y_train, X_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="output_33_0.png" alt="png"></p>
<p><img src="output_33_1.png" alt="png"></p>
<h1 id="Regularization-and-Coefficient-Analyse"><a href="#Regularization-and-Coefficient-Analyse" class="headerlink" title="Regularization and Coefficient Analyse"></a>Regularization and Coefficient Analyse</h1><p>Larger regularization will make $\theta$ more close to zero.<br>This time, risk shift from (close to dataset) to (close to regularization, far from dataset).<br>Approximation Error close to infinity, whereas estimation and optimization error close to zero  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">regularization_coefficient_analyse</span>(<span class="params">X_train, y_train, X_test, y_test</span>):</span><br><span class="line">    num_iter = <span class="number">10000</span></span><br><span class="line">    <span class="keyword">for</span> step_size <span class="keyword">in</span> [<span class="number">0.05</span>, <span class="number">0.01</span>]:</span><br><span class="line">        theta_hist_non_reg, _ = \</span><br><span class="line">            batch_grad_descent(X_train, y_train, step_size=step_size, num_iter=num_iter)</span><br><span class="line"></span><br><span class="line">        theta_sum: <span class="built_in">list</span> = []</span><br><span class="line">        lambda_list: <span class="built_in">list</span> = []</span><br><span class="line"></span><br><span class="line">        theta_sum.append(theta_hist_non_reg[-<span class="number">1</span>].<span class="built_in">sum</span>())</span><br><span class="line">        lambda_list.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> lambda_reg <span class="keyword">in</span> [<span class="number">1e-7</span>, <span class="number">1e-8</span>, <span class="number">1e-6</span>, <span class="number">1e-5</span>, <span class="number">1e-4</span>, <span class="number">1e-3</span>, <span class="number">1e-2</span>, <span class="number">1e-1</span>, <span class="number">1</span>, <span class="number">1e2</span>]:</span><br><span class="line">            theta_hist_reg, _ = \</span><br><span class="line">                regularized_grad_descent(X_train, y_train,</span><br><span class="line">                                         lambda_reg=lambda_reg, step_size=step_size, num_iter=num_iter)</span><br><span class="line"></span><br><span class="line">            theta_sum.append(theta_hist_reg[-<span class="number">1</span>].<span class="built_in">sum</span>())</span><br><span class="line">            lambda_list.append(lambda_reg)</span><br><span class="line"></span><br><span class="line">        plt.plot(lambda_list, theta_sum, label=<span class="string">&quot;step size: &#123;0&#125;&quot;</span>.<span class="built_in">format</span>(step_size))</span><br><span class="line">        plt.legend()</span><br><span class="line"></span><br><span class="line">    plt.xscale(<span class="string">&quot;log&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;regularization lambda&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;sum of coefficient of theta&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Theta constraned over regularization lambda&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">regularization_coefficient_analyse(X_train, y_train, X_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="output_35_0.png" alt="png"></p>
<h1 id="Risk-Minimization-Analyse"><a href="#Risk-Minimization-Analyse" class="headerlink" title="Risk Minimization Analyse"></a>Risk Minimization Analyse</h1><ol>
<li>show that for square loss $\mathcal{L}(\hat{y},y)=\frac{1}{2}(y-\hat{y}^{2})$ , its Bayes decision func is <script type="math/tex">f^{*}(X)=E[Y|X=x]</script><br>Risk Func <script type="math/tex">R(f)=\frac{1}{2}E[(f(x)-\hat{y})^{2}]=\frac{1}{2}E[y-\hat{y}^{2}|X=x]=\frac{1}{2}E[(y^{2}-2y\hat{y}+\hat{y}^{2})|X=x]=\frac{1}{2}E[y^{2}|X=x]-E[y\hat{y}|X=x]+\frac{1}{2}E[\hat{y}^{2}]</script><br>Therefore, <script type="math/tex">\frac{\partial R(f)}{\partial y}=E[y|X=x]-\hat{y}</script><br>To minimize $R(f)$, we get <script type="math/tex">f^{*}(X)=\hat{y}=E[Y|X=x]</script>  </li>
<li>show that for square loss <script type="math/tex">\mathcal{L}(\hat{y},y)=|y-\hat{y}|</script> , its Bayes decision func is <script type="math/tex">f^{*}(X)=median\ [Y|X=x]</script><br>Too hard, too many math!<br>Can reference: <a target="_blank" rel="noopener" href="https://github.com/kinslyzhu/DS-GA-1003-ML/blob/master/assignment/hw1-sgd/yz3079_hw1.ipynb">https://github.com/kinslyzhu/DS-GA-1003-ML/blob/master/assignment/hw1-sgd/yz3079_hw1.ipynb</a></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="/about/">Xiao Luo.</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab1%20Linear%20Descent,%20SGD,%20and%20Regularization/">http://example.com/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab1%20Linear%20Descent,%20SGD,%20and%20Regularization/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Note/">Note</a><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning</a></div><div class="post_share"><div class="social-share" data-image="/img/2.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/10/Course%20Note/Gatech/ECE%206607%20Computer%20Communication%20Network/"><img class="prev-cover" src="/img/1.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">ECE 6607 Computer Communication Network</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab2%20Lasso%20and%20Coordinate%20Descent/"><img class="next-cover" src="/img/3.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">DS-GA-1003 Lab2 Lasso and Coordinate Descent</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab2%20Lasso%20and%20Coordinate%20Descent/" title="DS-GA-1003 Lab2 Lasso and Coordinate Descent"><img class="cover" src="/img/3.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-10</div><div class="title">DS-GA-1003 Lab2 Lasso and Coordinate Descent</div></div></a></div><div><a href="/2022/12/10/Course%20Note/Gatech/ECE%206607%20Computer%20Communication%20Network/" title="ECE 6607 Computer Communication Network"><img class="cover" src="/img/1.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-10</div><div class="title">ECE 6607 Computer Communication Network</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xiao Luo.</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">82</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/luoxiao23333"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/luoxiao23333" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:luo.1632@osu.edu" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.linkedin.com/in/xiao-l-0489191a4/" target="_blank" title="Linkedin"><i class="fab fa-linkedin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Lab1-Gradient-Descent"><span class="toc-number">1.</span> <span class="toc-text">Lab1 Gradient Descent</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Import-and-Init-Dataset"><span class="toc-number">2.</span> <span class="toc-text">Import and Init Dataset</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Feature-Normalization"><span class="toc-number">3.</span> <span class="toc-text">Feature Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Wanrning"><span class="toc-number">3.1.</span> <span class="toc-text">Wanrning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Linear-Regression-with-Full-Batch"><span class="toc-number">4.</span> <span class="toc-text">Linear Regression with Full Batch</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Compute-Square-Loss"><span class="toc-number">4.1.</span> <span class="toc-text">Compute Square Loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Compute-Square-Loss-Gradient"><span class="toc-number">4.2.</span> <span class="toc-text">Compute Square Loss Gradient</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Check"><span class="toc-number">4.3.</span> <span class="toc-text">Gradient Check</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generic-Gradient-Checker"><span class="toc-number">4.4.</span> <span class="toc-text">Generic Gradient Checker</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Batch-Grad-Descent"><span class="toc-number">4.5.</span> <span class="toc-text">Batch Grad Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Draw-Convergence-Diagram"><span class="toc-number">4.6.</span> <span class="toc-text">Draw Convergence Diagram</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Note"><span class="toc-number">4.6.1.</span> <span class="toc-text">Note</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Ridge-Regression-with-L2-Regularized"><span class="toc-number">5.</span> <span class="toc-text">Ridge Regression with L2 Regularized</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Compute-Regularized-Square-Loss-Gradient"><span class="toc-number">5.1.</span> <span class="toc-text">Compute Regularized Square Loss Gradient</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regularized-Grad-Descent"><span class="toc-number">5.2.</span> <span class="toc-text">Regularized Grad Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Visualize-Ridge-Regression"><span class="toc-number">5.3.</span> <span class="toc-text">Visualize Ridge Regression</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Stochastic-Grad-Descent-SGD"><span class="toc-number">6.</span> <span class="toc-text">Stochastic Grad Descent(SGD)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-step-size-methods"><span class="toc-number">6.1.</span> <span class="toc-text">4 step size methods:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Visualize-SGD-by-4-methods"><span class="toc-number">6.2.</span> <span class="toc-text">Visualize SGD by 4 methods</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Compare-Full-Batch-and-SGD"><span class="toc-number">7.</span> <span class="toc-text">Compare Full Batch and SGD</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Overfit-Analyse"><span class="toc-number">8.</span> <span class="toc-text">Overfit Analyse</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Regularization-and-Coefficient-Analyse"><span class="toc-number">9.</span> <span class="toc-text">Regularization and Coefficient Analyse</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Risk-Minimization-Analyse"><span class="toc-number">10.</span> <span class="toc-text">Risk Minimization Analyse</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab1%20Linear%20Descent,%20SGD,%20and%20Regularization/" title="DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization"><img src="/img/2.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization"/></a><div class="content"><a class="title" href="/2022/12/10/Course%20Note/NYU/DS-GA-1003%20Lab1%20Linear%20Descent,%20SGD,%20and%20Regularization/" title="DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization">DS-GA-1003 Lab1 Linear Descent, SGD, and Regularization</a><time datetime="2022-12-11T02:03:25.000Z" title="Created 2022-12-10 21:03:25">2022-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/10/Algorithm/A%20Research%20Problem%EF%BC%88%E6%AC%A7%E6%8B%89%E5%87%BD%E6%95%B0%EF%BC%8CDFS%EF%BC%89/" title="A Research Problem（欧拉函数，DFS）"><img src="/img/4.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="A Research Problem（欧拉函数，DFS）"/></a><div class="content"><a class="title" href="/2022/12/10/Algorithm/A%20Research%20Problem%EF%BC%88%E6%AC%A7%E6%8B%89%E5%87%BD%E6%95%B0%EF%BC%8CDFS%EF%BC%89/" title="A Research Problem（欧拉函数，DFS）">A Research Problem（欧拉函数，DFS）</a><time datetime="2022-12-11T02:03:25.000Z" title="Created 2022-12-10 21:03:25">2022-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/10/Algorithm/Almost%20Prime%20Numbers%EF%BC%88%E7%B4%A0%E6%95%B0%E7%AD%9B%EF%BC%89/" title="Almost Prime Numbers（素数筛）"><img src="/img/6.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Almost Prime Numbers（素数筛）"/></a><div class="content"><a class="title" href="/2022/12/10/Algorithm/Almost%20Prime%20Numbers%EF%BC%88%E7%B4%A0%E6%95%B0%E7%AD%9B%EF%BC%89/" title="Almost Prime Numbers（素数筛）">Almost Prime Numbers（素数筛）</a><time datetime="2022-12-11T02:03:25.000Z" title="Created 2022-12-10 21:03:25">2022-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/10/Algorithm/A%20Spy%20in%20the%20Metro/" title="A Spy in the Metro"><img src="/img/5.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="A Spy in the Metro"/></a><div class="content"><a class="title" href="/2022/12/10/Algorithm/A%20Spy%20in%20the%20Metro/" title="A Spy in the Metro">A Spy in the Metro</a><time datetime="2022-12-11T02:03:25.000Z" title="Created 2022-12-10 21:03:25">2022-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/10/Algorithm/And%20Then%20There%20Was%20One/" title="And Then There Was One"><img src="/img/7.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="And Then There Was One"/></a><div class="content"><a class="title" href="/2022/12/10/Algorithm/And%20Then%20There%20Was%20One/" title="And Then There Was One">And Then There Was One</a><time datetime="2022-12-11T02:03:25.000Z" title="Created 2022-12-10 21:03:25">2022-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Xiao Luo.</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>